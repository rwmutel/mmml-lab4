{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMML Lab Assigment 4: Linear Dynamical Systems & Generative Modeling\n",
    "\n",
    "The aim of this lab assigment is to implement Kalman filter for two-dimensional data and observe its effect on real-world example of eye-tracking experiment as well as explore and implement generative modelling on examples of GANs and VAEs.\n",
    "\n",
    "We will discuss:\n",
    "* LDS sampling and whether Kalman filter indeed makes the improvement of the data correspondance\n",
    "* Theoretical background behind Generative Adversarial Networks (GANs)\n",
    "* Variational AutoEncoder (VAE) model and it's application to the MNIST distribution learning/generation\n",
    "\n",
    "Answering all questions will bring 7 points out of the total course grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions \n",
    "\n",
    "1.   Form a team of two\n",
    "2.   Rename the ipynb file to `Name1_Name2_MMML_Lab4.ipynb`\n",
    "3.   Indicate team members at the top\n",
    "4.   Provide your solutions (code or explanation) as necessary; do not reshuffle the cell order! You are welcome to optimize the existing commands if you want \n",
    "5.   Please execute all the cells before submission; make sure there are no errors, all plots have been generated, and all numerical answers calculated. Also, do not make *looong* prints\n",
    "6.   Submit your notebook to **cms** along with the pdf output.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pykalman\n",
    "from scipy import stats\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from typing import List, Tuple, OrderedDict\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "import io\n",
    "import ipywidgets as widgets\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lds(state, observations, filtered_observations = None):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x = state[:, 0], y = state[:, 1], mode = 'lines', name = \"Hidden (True) State\"))\n",
    "    fig.add_trace(go.Scatter(x = observations[:, 0], y = observations[:, 1], mode = 'lines', name = \"Observed (Measured) State\"))\n",
    "    if filtered_observations:\n",
    "        fig.add_trace(go.Scatter(x = filtered_observations[:, 0], y = filtered_observations[:, 1], mode = 'lines', name = \"Filtered Observed State\"))\n",
    "    fig.update_layout(title='LDS')\n",
    "    return fig\n",
    "\n",
    "def plot_correlation_lds(state, observations, annotation):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x = state[:, 0], y = observations[:, 0], mode = 'markers', name = \"1st dimension\"))\n",
    "    fig.add_trace(go.Scatter(x = state[:, 1], y = observations[:, 1], mode = 'markers', name = \"2nd dimension\"))\n",
    "    fig.update_layout(title='Correlation', yaxis_title = annotation, xaxis_title = \"Hidden states\")\n",
    "    fig.update_xaxes(scaleanchor = \"y\", scaleratio = 1, constrain = \"domain\")\n",
    "    return fig\n",
    "\n",
    "def load_eyetracking_data(data_fname):\n",
    "\n",
    "  with np.load(data_fname, allow_pickle=True) as dobj:\n",
    "    data = dict(**dobj)\n",
    "\n",
    "  images = [plt.imread(io.BytesIO(stim), format='JPG')\n",
    "            for stim in data['stimuli']]\n",
    "  subjects = data['subjects']\n",
    "\n",
    "  return subjects, images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 2D Linear Dynamical Systems (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Kalman Filter for 2D LDS (1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that Linear Dynamical System is a special case of State Space Model (SSM) where hidden states are continuous. In general, the dynamics of the system are described via the following equations:\n",
    "\n",
    "$$\\mathbf{z}_{n} = \\mathbf{A}\\mathbf{z}_{n-1} + \\mathbf{w}_{n}$$\n",
    "$$\\mathbf{x}_{n} = \\mathbf{C}\\mathbf{z}_{n} + \\mathbf{v}_{n}$$\n",
    "$$\\mathbf{z}_{1} = \\mathbf{\\mu}_{0} + \\mathbf{u}$$\n",
    "\n",
    "where $\\mathbf{w}_{n} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Gamma})$, $\\mathbf{v}_{n} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma})$, $\\mathbf{u} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{V}_{0})$ all being Gaussian noises with zero means.\n",
    "\n",
    "Here, $\\mathbf{z}_{n}$ is a hidden state (true value); observe that it depends only on the previous state $\\mathbf{z}_{n-1}$ plus some process-dependent Gaussian noise. Then, $\\mathbf{x}_{n}$, the observed value, is a linear transformation of hidden one, $\\mathbf{z}_{n}$, plus some measurement-dependent Gaussian noise. The initial hidden state is also a subject to the noise $\\mathbf{u}$.\n",
    "\n",
    "Kalman filter is the algorithm which allows us to correct for the prediction of the following true value by incorporating knowledge about the measurement. Suppose, we know $\\mathbf{z}_{n-1}$, then the best prediction (prior) we can make about the next hidden state is $\\mathbf{A}\\mathbf{z}_{n-1}$ but we also have another source of the information - our measurement (observation) $\\mathbf{x}_{n}$; Kalman filter enables us with efficient integration of both information sources to get the best prediction we can do. In the very end of the derivations, it tell us that:\n",
    "\n",
    "$$\\mathbf{\\mu}_{n} = \\mathbf{A}\\mathbf{\\mu}_{n-1} + \\mathbf{K}_{n}(\\mathbf{x}_{n} - \\mathbf{C} \\mathbf{A} \\mathbf{\\mu}_{n-1})$$\n",
    "$$\\mathbf{V}_{n} = (\\mathbf{I} - \\mathbf{K}_{n}\\mathbf{C})\\mathbf{P}_{n-1}$$\n",
    "\n",
    "where $\\mathbf{K}_{n}$ is a Kalman gain which basically tells us how much of the attention we should pay to the prior (dynamical system prediction) vs to the likelihood (observation) depending on the values of the noise:\n",
    "$$\\mathbf{K}_{n} = \\mathbf{P}_{n-1}\\mathbf{C}^{T}(\\mathbf{C}\\mathbf{P}_{n-1}\\mathbf{C}^{T} + \\mathbf{\\Sigma}^{-1})$$\n",
    "\n",
    "One more variable to be discussed here and it is $\\mathbf{P}_{n-1}$ matrix which corresponds to the noise of the posterior distribution of $\\mathbf{z}_{n}$ after applying dynamics of the system (but before incorporating the likelihood!). It is calculated as following:\n",
    "\n",
    "$$\\mathbf{P}_{n-1} = \\mathbf{A}\\mathbf{V}_{n-1}\\mathbf{A}^{T} + \\mathbf{\\Gamma}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Sampling from 2D LDS (0.25 pts)\n",
    "\n",
    "In the exercise below you are suggested to sample from the LDS and observe the differnce between hidden and measured states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 2\n",
    "\n",
    "# initialize model parameters\n",
    "params = {\n",
    "  'n_dim': 2, # dimensionality of the hidden and observed states (in general, not necessarily equal!)\n",
    "  'A': 0.9 * np.eye(n_dim),  # state transition matrix\n",
    "  'Gamma': np.eye(n_dim),  # state noise covariance\n",
    "  'C': np.eye(n_dim),  # observation matrix\n",
    "  'Sigma': 1.0 * np.eye(n_dim),  # observation noise covariance\n",
    "  'mu_0': np.zeros(n_dim),  # initial state mean\n",
    "  'V_0': 0.1 * np.eye(n_dim),  # initial state noise covariance\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def sample_lds(n_timesteps, params):\n",
    "  \"\"\" Generate samples from a Linear Dynamical System specified by the provided\n",
    "  parameters.\n",
    "\n",
    "  Args:\n",
    "  n_timesteps (int): the number of time steps to simulate\n",
    "  params (dict): a dictionary of model parameters\n",
    "\n",
    "  Returns:\n",
    "  ndarray, ndarray: the generated state and observation data\n",
    "  \"\"\"\n",
    "\n",
    "  state = np.zeros((n_timesteps, n_dim))\n",
    "  observations = np.zeros((n_timesteps, n_dim))\n",
    "\n",
    "  # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "  # ========== YOUR CODE ENDS HERE ========== #\n",
    "\n",
    "\n",
    "  return state, observations\n",
    "\n",
    "\n",
    "n_timesteps = 100\n",
    "state, observations = sample_lds(n_timesteps, params)\n",
    "\n",
    "fig = plot_lds(state, observations)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that observation matrix for this particular LDS is identity matrix, meaning that we expect for the perfect correspondence between hidden and observed states (not counting for the noise). Let us check for it by plotting correlation between each of the dimensions for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_correlation_lds(state, observations, annotation = \"Measurements\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not the perfect line!\n",
    "\n",
    "Let us also quantify the discrepancy induced by the noise; for that we will calculate R-squared score (observe the way we do so; you will have discussion question in the end of the exercise for that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R2-score for 1st dimension of observations is {r2_score(state[:, 0], observations[:, 0]):.02f}\")\n",
    "print(f\"R2-score for 2nd dimension of observations is {r2_score(state[:, 1], observations[:, 1]):.02f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Kalman Filter for 2D Data (0.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to implement Kalman filter for the given system (well. your function should be generalized version for any set of parameters and any dimensionality) and observe whether it improves the situation with the predictability of the true state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_filter(observations, params):\n",
    "  \"\"\" Perform Kalman filtering (forward pass) on the data given the provided\n",
    "  system parameters.\n",
    "\n",
    "  Args:\n",
    "    observations (ndarray): a sequence of observations of shape(n_timesteps, n_dim)\n",
    "    params (dict): a dictionary of model parameters\n",
    "\n",
    "  Returns:\n",
    "    ndarray, ndarray: the filtered system means and noise covariance values\n",
    "  \"\"\"\n",
    "  # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "  # ========== YOUR CODE ENDS HERE ========== #\n",
    "\n",
    "  return mu, sigma\n",
    "\n",
    "\n",
    "filtered_state_means, filtered_state_covariances = kalman_filter(observations, params)\n",
    "\n",
    "fig = plot_correlation_lds(state, filtered_state_means, annotation = \"Filtered measurements\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also calculate R-squared score here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R2-score for 1st dimension of observations is {r2_score(state[:, 0], filtered_state_means[:, 0]):.02f}\")\n",
    "print(f\"R2-score for 2nd dimension of observations is {r2_score(state[:, 1], filtered_state_means[:, 1]):.02f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 R-squared improvement (0.25 pts)\n",
    "\n",
    "Why do we calculate R-squared value explicitly on this arrays of data? Will we have the same way to do that if matrix $\\mathbf{C}$ is no longer identity one? \n",
    "\n",
    "Why do you think R-squared value has improved? Do we expect it to increase after filtering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Real-world application of Kalman Filter (1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kalman Filter is an extremely widely-used method in online application where data points drop to us one by one and we would like somehow to filter for outliers and smooth the general data. In this part of the lab task, you are going to fit 2D LDS using EM-method already implemented in the library `pykalman` for the eye-tracking dataset where people watched the images and location of their gaze was measured (of course, these measurements are subjects to noise as it is pretty hard to reliably estimate the exact gaze fixation position, even for us, as we move our eyes a lot in an unconscious manner).\n",
    "\n",
    "We will take a look at 3 images and 5 different subjects watching them. Each subject fixated in the center of the screen before the image appeared, then had a few seconds to freely look around. At first, let us read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects, images = load_eyetracking_data(\"eyetracking.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the widget below you can explore eyetracking data gathered for different subjects and images (if you want to see clear image without data, please choose `subject_id = -1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28b97752f2849f28b415e1ff55744c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=-1, description='subject_id', max=4, min=-1), IntSlider(value=0, descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@widgets.interact(subject_id=widgets.IntSlider(-1, min=-1, max=4),\n",
    "                  image_id=widgets.IntSlider(0, min=0, max=2))\n",
    "def plot_subject_trace(subject_id=-1, image_id=0):\n",
    "  if subject_id == -1:\n",
    "    subject = np.zeros((3, 0, 2))\n",
    "  else:\n",
    "    subject = subjects[subject_id]\n",
    "  data = subject[image_id]\n",
    "  img = images[image_id]\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.imshow(img, aspect='auto')\n",
    "  ax.scatter(data[:, 0], data[:, 1], c='m', s=100, alpha=0.7)\n",
    "  ax.set(xlim=(0, img.shape[1]), ylim=(img.shape[0], 0))\n",
    "  plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Fitting data with `pykalman` (0.25 pts)\n",
    "\n",
    "Learning algorithm for LDS is too complex to be covered during our course. Thus, we will simply use the external library which will help us to fit the parameters using EM-method. Observe that we will provide initial state mean and covariance manually, thus we will fit all other parameters (you need to specify them in `em_vars`) using EM (here is the [link](https://pykalman.github.io/) to the documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= YOUR CODE STARTS HERE ========= #\n",
    "kf = pykalman.KalmanFilter(\n",
    "  n_dim_state=...,\n",
    "  n_dim_obs=...,\n",
    "  em_vars=...\n",
    ")\n",
    "# ========== YOUR CODE ENDS HERE ========== #\n",
    "\n",
    "subject_id = 1\n",
    "image_id = 1\n",
    "data = subjects[subject_id][image_id]\n",
    "\n",
    "kf.initial_state_mean = data[0]\n",
    "kf.initial_state_covariance = 0.1*np.eye(2)\n",
    "kf.em(data)\n",
    "\n",
    "# ========= YOUR CODE STARTS HERE ========= #\n",
    "print(f'A =\\n{...}')\n",
    "print(f'Gamma =\\n{...}')\n",
    "print(f'C =\\n{...}')\n",
    "print(f'Sigma =\\n{...}')\n",
    "# ========== YOUR CODE ENDS HERE ========== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question to be answered**: What can you say about the process while taking a look at the fitted dynamical parameters?\n",
    "\n",
    "*Your thoughts go here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Custom Kalman filter vs `pykalman.filter()` (0.25 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare the results of our custom Kalman filter implemeted earlier and the one proposed in `pykalman` filter. For that, we need to prepare fitted parameters in the corresponding format to use our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= YOUR CODE STARTS HERE ========= #\n",
    "params = {\n",
    "\n",
    "}\n",
    "# ========== YOUR CODE ENDS HERE ========== #\n",
    "\n",
    "filtered_state_means_custom, filtered_state_covariances_custom = kalman_filter(data, params)\n",
    "\n",
    "# ========= YOUR CODE STARTS HERE ========= #\n",
    "filtered_state_means_pykalman, filtered_state_covariances_pykalman = ...\n",
    "\n",
    "# ========== YOUR CODE ENDS HERE ========== #\n",
    "\n",
    "print(f\"Wow, we implemented Kalman filter correctly: {np.allclose(filtered_state_means_custom, filtered_state_means_pykalman)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Discussion points (0.5 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we would like you to think about the experience you've just had so that you could get the broad picture of Kalman filter usage. Please answer the following questions (some of them will require you to do the small experiment to justify your answer):\n",
    "\n",
    "1. Do you think that Linear Dynamical System is the right choice for the modeling the underlying dynamics of eye gazing? Why yes / why no?\n",
    "2. Run the Kalman filter on the fitted parameters for other subjects and images as well, what do you think of its quality (calculate some metrics to justify your ideas)? Why do you think it is the way it is (performs poorly or generalizes well - depending what you have obtained)?\n",
    "3. Sample from Kalman filter - what do you think of predicitve power for our system? Does it make sense to tell that we have developed the tool which is able to say where the person is doing to look in the next time point? Which of the factros have more impact - prior or measurement (hint: you may want to look at Kalman gain value)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Kalman smoother (additional, up to 1.5 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we are presented with the whole sequence (but not online updates), we can benfit from backward pass of the data, as the future states also can tell us about the nature of previous ones. Analogue of Backward algorithm for LDS is Kalman smoother. This additional task requires you to implement your own Kalman smoother (0.5 pts) as well as write down mathematical structures which are necessary for this implementation (0.5 pts). Then, please compare the results of Kalman filter and Kalman smoother, as $\\mathbf{C}$ isn't the identity matrix (up to certain level of confidence), what other metric you would suggest using to compare their work (0.5 pts)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "# ========== YOUR CODE ENDS HERE ========== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generative modelling (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Generative Adversarial Networks (GANs) (1.25 pts)\n",
    "\n",
    "GANs is a very interesting, yet powerful approach to the generative modelling. In the GANs paradigm, we learn data distribution $p_{data}$ implicitly - via an adversarial training between two networks: Generator $G$ and Discriminator $D$. We aim to make generator distribution $p_g$ a good approximation to the data distribution $p_{data}$. The generator distribution is learned as mapping from some simple prior distribution $p_Z(z)$ to the data. Later, our Discriminator takes as input true samples (from $p_{data}$) and generated samples (from $p_g$) and tries to differentiate between them. As output of our Discriminator, we expect to see the probability of sample being from the data distribution ($p_{data}$). Discriminator tries to correctly guess whether samples is true or fake, and Generator tries to fool the Discriminator by creating realistic samples. \n",
    "\n",
    "More formally, $D$ and $G$ play the following two-player minimax game with value function $V(G, D)$:\n",
    "\n",
    "$$\n",
    "\\min_G \\max_D V(G, D) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_Z(z)}[\\log(1 - D(G(z)))]\n",
    "$$\n",
    "\n",
    "In this section, we'll analyze why $p_g = p_{data}$ is the global optimum of the objective above (which is good, as that's what we want our generator to learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Optimal discriminator (0.5 pts)\n",
    "\n",
    "Prove that for fixed $G$ optimal discriminator $D$ is:\n",
    "\n",
    "$$\n",
    "D_G^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we have optimal discriminator $D_G^*(x)$, the objective for generator $C(G)$ can be defined as follows:\n",
    "\n",
    "$$\n",
    "C(G) = \\max_D V(G, D) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D_G^*(x)] + \\mathbb{E}_{z \\sim p_Z(z)}[\\log(1 - D_G*(G(z)))]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D_G^*(x)] + \\mathbb{E}_{x \\sim p_g(x)}[\\log(1 - D_G^*(x))]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\mathbb{E}_{x \\sim p_{data}(x)}[\\log \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}] + \\mathbb{E}_{x \\sim p_g(x)}[\\log \\frac{p_g(x)}{p_{data}(x) + p_g(x)}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Proof\n",
    "\n",
    "Optimal $D^*_G(x)$ maximizes $C(G)$. Using the definition of the expected value, $C(G)$ can be rewritten as $$\\int_x{[p_{data}(x)log(D^*_G(x)) + p_{g}(x)log(1-D^*_G(G(x)))]dx} $$\n",
    "\n",
    "To maximize this objective, it is sufficient to find $D^*_G(x)$ that maximizes $p_{data}(x)log(D^*_G(x)) + p_{g}(x)log(1-D^*_G(G(x)))$ under the integral. It is an expression of type $$f(x) = a\\log{(x)} + b\\log{(1-x)}$$. Its derivative is equal to $$f(x)'_x=\\frac{a}{x} - \\frac{b}{1-x} = \\frac{a-ax-bx}{x-x^2}$$. Assuming $x \\in (0,1)$ (since the output of $D^*_G(x)$ is likelihood of x coming from the real distribution), there is only one critical point where the derivative is equal to zero:\n",
    "$$a-ax^*-bx^*=0 $$\n",
    "$$ x^*(a+b) = a $$\n",
    "$$ x^* = \\frac{a}{a+b}$$\n",
    "\n",
    "The second derivative of $f(x)$ is $$f(x)''_x = -\\frac{a}{x^2} - \\frac{b}{(1-x)^2} = - (\\frac{a}{x^2} + \\frac{b}{(1-x)^2})$$\n",
    "which is always negative for $a, b > 0$, thus the funciton is concave.\n",
    "Therefore, the critical point $x^* = \\frac{a}{a+b}$ is the point of global maxima of $f(x) = a\\log{(x)} + b\\log{(1-x)}$ and thus, the such $D^*_G(x)$ that maximizes $p_{data}(x)log(D^*_G(x)) + p_{g}(x)log(1-D^*_G(G(x)))$ is equal to $$\\frac{p_{data}(x)}{p_{data}(x)+p_g(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Optimal generator (0.5 pts)\n",
    "\n",
    "Prove that the global minimum of the generator objective $C(G)$ (when we have optimal discriminator) is achieved if and only if $p_g = p_{data}$. Show that at this point, $C(G)$ achieves value $-\\log 4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For optimal discriminator $D^*_G(x)=\\frac{p_{data}}{p_{data}+p_g}$ we have $C(G) = \\mathbb{E}_{x ~ p_{data}}[\\log(\\frac{1}{2})] + \\mathbb{E}_{x ~ p_g}[\\log(1-\\frac{1}{2})] = \\log(\\frac{1}{2} * \\frac{1}{2}) = -\\log4$.\n",
    "\n",
    "Let us also proof the statement backwards (if the value is equal to $\\log4$, it is optimal)\n",
    "\n",
    "$$C(G) = \\int_x [p_{data}\\log(\\frac{p_{data}}{p_g+p_{data}}) + p_{g}\\log(1-\\frac{p_{data}}{p_g+p_{data}})]dx =$$\n",
    "$$\\int_x [p_{data}\\log(\\frac{p_{data}}{p_g+p_{data}}) + p_{g}\\log(\\frac{p_g}{p_g+p_{data}})]dx =$$\n",
    "$$\\int_x [(\\log2-\\log2)(p_{data}+p_g) + p_{data}\\log(\\frac{p_{data}}{p_g+p_{data}}) + p_{g}\\log(\\frac{p_g}{p_g+p_{data}})]dx =$$\n",
    "$$-\\log2 * \\int_x [p_{data}+p_g]dx + \\int_x [p_{data}(\\log2+\\log(\\frac{p_{data}}{p_g+p_{data}})) + p_{g}(\\log2+\\log(\\frac{p_g}{p_g+p_{data}}))]dx =$$\n",
    "$$-2\\log2 + \\int_x [p_{data}(\\log\\frac{p_{data}}{\\frac{p_g+p_{data}}{2}}) + p_g(\\log\\frac{p_g}{\\frac{p_g+p_{data}}{2}})]dx = $$\n",
    "$$-\\log4 + \\int_x [p_{data}(\\log\\frac{p_{data}}{\\frac{p_g+p_{data}}{2}})]dx + \\int_x [p_{g}(\\log\\frac{p_{g}}{\\frac{p_g+p_{data}}{2}})]dx = $$\n",
    "$$-\\log4 + KL(p_{data} || \\frac{p_{data}+p_g}{2}) + KL(p_{g} || \\frac{p_{data}+p_g}{2}) = $$\n",
    "$$-\\log4 + 2 * JSD(p_{data}||p_g)$$\n",
    "since Jensen-Shannon divergence is always non-negative, and equals zero if and only if both of the distributions are the same, the optimal value of $C(G)$ is not less than $-\\log4$ and is obtained in point where $p_{data} = p_g$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Summary (0.25 pts)\n",
    "\n",
    "* From the theorems proven above, how would you train your GANs model? Would we converge to the $p_g = p_{data}$ with such training?\n",
    "\n",
    "To achieve convergence we must train our generator and discriminator simultaneously or at least balanced, in order not to let the generator exploit some outlier data points which are hard for discriminator to distinguish from fake data points and soon **mode collapse** will happen i.e. the generator will focus **only** on the those exploitable outlier datapoints.\n",
    "\n",
    "Also, we would like to stop when the loss value is not decreasing anymore, since it indicates that discriminator is \"perfect\" and the generator does not actually improves since it cannot \"win\" the discriminator. We can evaluate the performance of out generator by looking how close the loss is to $-\\log4$\n",
    "\n",
    "* Summarize in a few sentences what you have learned and achieved by completing the tasks of this assignment. Comment on how this assignment can be imporved in the future.\n",
    "\n",
    "It was a very interesting task to understand how math proofs from *relatively* recent ML papers actually work. \n",
    "\n",
    "It is also interesting how game theory framework is applied to the task of generative models and how some of the constants, such as optimal value to which $C(G)$ converges, can be derived in closed form.\n",
    "\n",
    "Also, the paper on GAN itself is written nicely, except of it was not really clear how \"by subtracting this expression from C(G) = V (D∗G, G), we obtain *an equation with two KL-divergences introduced*\". However, the paper is popular, so it was not hard to look for additional explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Variational AutoEncoders (VAEs) (3.75 pts)\n",
    "\n",
    "Variational AutoEncoder (VAE) is a specific type of Latent Variable Model (LVM). In LVM paradigm, we assume that our data $x$ can be modelled via some hidden (unobserved) and semantically meaningful variable $z$. VAE is defined in the following way:\n",
    "\n",
    "* latent variables $z$ have a standard normal prior $p_Z(z) = \\mathcal{N}(0, I)$\n",
    "* an approximate posterior $q_{\\phi}(z|x) = \\mathcal{N}(\\mu_{\\phi}(x), \\Sigma_{\\phi}(x))$, where $\\mu_{\\phi}(x)$ is the mean vector and $\\Sigma_{\\phi}(x)$ is a diagonal covariance matrix\n",
    "* a decoder $p_{\\theta}(x|z) = \\mathcal{N}(\\mu_{\\theta}(z), \\Sigma_{\\theta}(z))$, where $\\mu_{\\theta}(z)$ is the mean vector and $\\Sigma_{\\theta}(z)$ is a diagonal covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 VAEs on 2D data (2 pts)\n",
    "\n",
    "In this question, you will train a simple VAE on 2D data.\n",
    "\n",
    "Firstly, let's generate and visualize our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_2d_data(count: int) -> np.ndarray:\n",
    "    rand = np.random.RandomState(0)\n",
    "    samples = [[1.0, 2.0]] + (rand.randn(count, 2) * [[5.0, 1.0]]).dot(\n",
    "        [[np.sqrt(2) / 2, np.sqrt(2) / 2], [-np.sqrt(2) / 2, np.sqrt(2) / 2]])\n",
    "    return samples.astype(np.float32)\n",
    "\n",
    "train_data, test_data = sample_2d_data(10000), sample_2d_data(2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_2d_data(train_data: np.ndarray, test_data: np.ndarray):\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.set_title('Train Data')\n",
    "    ax1.scatter(train_data[:, 0], train_data[:, 1])\n",
    "    ax2.set_title('Test Data')\n",
    "    ax2.scatter(test_data[:, 0], test_data[:, 1])\n",
    "    plt.show()\n",
    "\n",
    "visualize_2d_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some utils functions, which will be used later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vae_training_plot(train_losses: np.ndarray, test_losses: np.ndarray, title: str):\n",
    "    vlb_train, recon_train, kl_train = train_losses[:, 0], train_losses[:, 1], train_losses[:, 2]\n",
    "    vlb_test, recon_test, kl_test = test_losses[:, 0], test_losses[:, 1], test_losses[:, 2]\n",
    "    plt.figure()\n",
    "    n_epochs = len(test_losses) - 1\n",
    "    x_train = np.linspace(0, n_epochs, len(train_losses))\n",
    "    x_test = np.arange(n_epochs + 1)\n",
    "\n",
    "    plt.plot(x_train, vlb_train, label='-vlb_train')\n",
    "    plt.plot(x_train, recon_train, label='recon_loss_train')\n",
    "    plt.plot(x_train, kl_train, label='kl_loss_train')\n",
    "    plt.plot(x_test, vlb_test, label='-vlb_test')\n",
    "    plt.plot(x_test, recon_test, label='recon_loss_test')\n",
    "    plt.plot(x_test, kl_test, label='kl_loss_test')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "def scatter_2d(data: np.ndarray, title: str):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.scatter(data[:, 0], data[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1.1 VAE Encoder and Decoder (0.25 pts)\n",
    "\n",
    "Implement VAE Encoder and Decoder. Our latent variables $z$ dimensionality here will be 2 (for visualizations). The recommended architecture is following:\n",
    "\n",
    "```\n",
    "linear(in_dim, out_dim)\n",
    "relu()\n",
    "\n",
    "Encoder\n",
    "    linear(2, 128)\n",
    "    relu()\n",
    "    linear(128, 128)\n",
    "    relu() \n",
    "    linear(128, 2*2) # 2 - for mu and 2 - for sigma\n",
    "\n",
    "Decoder\n",
    "    linear(2, 128)\n",
    "    relu()\n",
    "    linear(128, 128)\n",
    "    relu() \n",
    "    linear(128, 2*2) # 2 - for mu and 2 - for sigma\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_shape: int, output_shape: int, hiddens: List[int]):\n",
    "        super().__init__()\n",
    "        # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "        # ========== YOUR CODE ENDS HERE ========== #\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "        # ========== YOUR CODE ENDS HERE ========== #\n",
    "        return output\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_shape: int, output_shape: int, hiddens: List[int]):\n",
    "        super().__init__()\n",
    "        # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "        # ========== YOUR CODE ENDS HERE ========== #\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "        # ========== YOUR CODE ENDS HERE ========== #\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1.2 KL-divergence between 2 Gaussian Distributions (0.5 pts)\n",
    "\n",
    "Recall that KL-divergence between 2 arbitratry distributions $p$ and $q$ is defined as follows:\n",
    "\n",
    "$$\n",
    "D_{KL}(p \\parallel q) = \\mathbb{E}_{x \\sim p(x)} \\log \\frac{p(x)}{q(x)}\n",
    "$$\n",
    "\n",
    "When our distributions $p$ and $q$ are Multivariate Gaussians ($p = \\mathcal{N}(\\mu_p, \\Sigma_p)$, $q = \\mathcal{N}(\\mu_q, \\Sigma_q)$) in $d$ dimensions, derive the formula for KL-divergence between them. \n",
    "\n",
    "Simplify the formula for $q$ being Standard Gaussian ($q = \\mathcal{N}(0, I)$).\n",
    "\n",
    "Hint: For $x \\sim \\mathcal{N}(\\mu, \\Sigma), E[(x-m)A(x-m)^T] = (\\mu-m)A(\\mu-m)^T + \\operatorname{trace}(A\\Sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1.3 VAE implementation (0.5 pts)\n",
    "\n",
    "Implement the code for VAE below:\n",
    "* output shape of encoder and decoder should be $2 * \\text{latent\\_dim}$ and $2*\\text{input\\_dim}$ respectively as we predict both $\\mu$ and $\\Sigma$\n",
    "* as our $\\Sigma$ is diagonal (by design) and must be positive-semidefinite, we predict logarithm of $\\Sigma$ and later exponentiate it \n",
    "* in this task, our Decoder also produces Gaussian distribution, from which we should sample our data point $x$ (or calculate probability of seeing it in the reconstruction loss).\n",
    "* when computing reconstruction loss and KL loss, average over the batch dimension and **sum** over the feature dimension\n",
    "\n",
    "Our loss is defined as (minus) Variational Lower Bound (VLB):\n",
    "\n",
    "$$\n",
    "VLB = \\mathbb{E}_{z \\sim q_{\\phi}(z|x)}\\log p_{\\theta}(x|z) - D_{KL}(q_{\\phi}(z|x) \\parallel p_Z(z))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedVAE(nn.Module):\n",
    "    def __init__(self, input_dim: int, latent_dim: int, enc_hidden_sizes: List[int]=[],\n",
    "                 dec_hidden_sizes: List[int]=[]):\n",
    "        super().__init__()\n",
    "        # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "        # ========== YOUR CODE ENDS HERE ========== #\n",
    "        \n",
    "    def reconstruction_loss(self, x: torch.Tensor, mu_x: torch.Tensor, log_std_x: torch.Tensor) -> torch.Tensor: \n",
    "        # compute reconstruction loss as logprob of x\n",
    "        # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "        # ========== YOUR CODE ENDS HERE ========== #\n",
    "        return recon_loss\n",
    "    \n",
    "    def kl_loss(self, mu_z: torch.Tensor, log_std_z: torch.Tensor) -> torch.Tensor:\n",
    "        # compute kl divergence between approximate posterior N(mu_z, Sigma_z) and prior N(0, I)\n",
    "        # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "        # ========== YOUR CODE ENDS HERE ========== #\n",
    "        return kl_loss\n",
    "    \n",
    "    def loss(self, x: torch.Tensor) -> OrderedDict[str, torch.Tensor]:\n",
    "        # calculate loss as sum of reconstruction and kl-divergence losses\n",
    "        # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "        # ========== YOUR CODE ENDS HERE ========== #\n",
    "        return OrderedDict(loss=loss, recon_loss=recon_loss,\n",
    "                           kl_loss=kl_loss)\n",
    "    \n",
    "    def sample(self, n: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        # implement sampling\n",
    "        # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "        # ========== YOUR CODE ENDS HERE ========== #\n",
    "        return z, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1.4 VAE training (0.5 pts)\n",
    "\n",
    "Train VAE on the generated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model: nn.Module, train_loader: DataLoader, optimizer: optim) -> OrderedDict[str, List[torch.Tensor]]:\n",
    "    # implement training of VAE for one epoch\n",
    "    # you should return dict with history of all losses during this epoch\n",
    "    model.train()\n",
    "    losses = OrderedDict()\n",
    "    # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "    # ========== YOUR CODE ENDS HERE ========== #\n",
    "    return losses\n",
    "\n",
    "\n",
    "def eval(model: nn.Module, data_loader: DataLoader) -> OrderedDict[str, torch.Tensor]:\n",
    "    # implement evaluation of VAE after epoch\n",
    "    # you should return dict with average losses over whole test dataset\n",
    "    model.eval()\n",
    "    total_losses = OrderedDict()\n",
    "    with torch.no_grad():\n",
    "        # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "        # ========== YOUR CODE ENDS HERE ========== #\n",
    "    return total_losses\n",
    "\n",
    "\n",
    "def train(model: nn.Module, train_loader: DataLoader, test_loader: DataLoader, epochs: int, lr: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses, test_losses = OrderedDict(), OrderedDict()\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"EPOCH {epoch} started\")\n",
    "        model.train()\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer)\n",
    "        test_loss = eval(model, test_loader)\n",
    "\n",
    "        for k in train_loss.keys():\n",
    "            if k not in train_losses:\n",
    "                train_losses[k] = []\n",
    "                test_losses[k] = []\n",
    "            train_losses[k].extend(train_loss[k])\n",
    "            test_losses[k].append(test_loss[k])\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training routine\n",
    "model = FullyConnectedVAE(2, 2, [128, 128], [128, 128])\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=128)\n",
    "train_losses, test_losses = train(model, train_loader, test_loader, epochs=10, lr=1e-3)\n",
    "train_losses = np.stack((train_losses['loss'], train_losses['recon_loss'], train_losses['kl_loss']), axis=1)\n",
    "test_losses = np.stack((test_losses['loss'], test_losses['recon_loss'], test_losses['kl_loss']), axis=1)\n",
    "plot_vae_training_plot(train_losses, test_losses, title=\"Train plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1.5 VAE Sampling (0.25 pts)\n",
    "\n",
    "Sample 1000 values using your VAE model. Visualize both latent and generated variables. Compare distribution of generated values with data distribution. Comment on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "# ========== YOUR CODE ENDS HERE ========== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 VAEs on Images (1.5 pts)\n",
    "\n",
    "In this question, you will train CNN (Convolutional Neural Network) version of VAE on image dataset (SVHN) and analyze it properties. \n",
    "\n",
    "Please download dataset from [here](https://drive.google.com/drive/folders/1ICMh3Zz2hG23WcrjYANRk14Iqm8lC0I9?usp=sharing).\n",
    "\n",
    "Firstly, let's load and visualize our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_data(fname: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    with open(fname, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    train_data, test_data = data['train'], data['test']\n",
    "    if 'mnist.pkl' in fname or 'shapes.pkl' in fname:\n",
    "        train_data = (train_data > 127.5).astype('uint8')\n",
    "        test_data = (test_data > 127.5).astype('uint8')\n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = load_image_data(\"data/svhn.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_images(images: np.ndarray):\n",
    "    images = (torch.FloatTensor(images) / 255).permute(0, 3, 1, 2)\n",
    "    grid_img = make_grid(images, nrow=10)\n",
    "    plt.figure()\n",
    "    plt.title(\"Images\")\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_image_dataset(data: np.ndarray):\n",
    "    idxs = np.random.choice(len(data), replace=False, size=(100,))\n",
    "    images = data[idxs]\n",
    "    visualize_images(images)\n",
    "\n",
    "visualize_image_dataset(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2.1 CNN VAE (0.5 pts)\n",
    "\n",
    "Implement CNN VAE. Our latent variables $z$ dimensionality here will be 16. For a decoder $p_{\\theta}(x|z) = \\mathcal{N}(\\mu_{\\theta}(z), \\Sigma_{\\theta}(z))$ we will assume that $\\Sigma_{\\theta}(z) = I$ (we are not learning covariance of the decoder). The recommended architecture is following:\n",
    "\n",
    "```\n",
    "conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "transpose_conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "linear(in_dim, out_dim)\n",
    "relu()\n",
    "flatten()\n",
    "\n",
    "Encoder\n",
    "    conv2d(3, 32, 3, 1, 1)\n",
    "    relu()\n",
    "    conv2d(32, 64, 3, 2, 1) # 16 x 16\n",
    "    relu() \n",
    "    conv2d(64, 128, 3, 2, 1) # 8 x 8\n",
    "    relu()\n",
    "    conv2d(128, 256, 3, 2, 1) # 4 x 4\n",
    "    relu()\n",
    "    flatten() # 16\n",
    "    linear(4 * 4 * 256, 2 * 16) # 16 - for mu and 16 - for sigma\n",
    "\n",
    "Decoder\n",
    "    linear(16, 4 * 4 * 128)\n",
    "    relu()\n",
    "    reshape(4, 4, 128)\n",
    "    transpose_conv2d(128, 128, 4, 2, 1) # 8 x 8\n",
    "    relu()\n",
    "    transpose_conv2d(128, 64, 4, 2, 1) # 16 x 16\n",
    "    relu()\n",
    "    transpose_conv2d(64, 32, 4, 2, 1) # 32 x 32\n",
    "    relu()\n",
    "    conv2d(32, 3, 3, 1, 1)\n",
    "```\n",
    "\n",
    "*   when computing reconstruction loss, it suffices to just compute MSE between the reconstructed $x$ and true $x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int, output_shape: Tuple[int, int, int]):\n",
    "        super().__init__()\n",
    "        # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "        # ========== YOUR CODE ENDS HERE ========== #\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "        # ========== YOUR CODE ENDS HERE ========== #\n",
    "        return output\n",
    "\n",
    "\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, input_shape: Tuple[int, int, int], latent_dim: int):\n",
    "        super().__init__()\n",
    "        # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "        # ========== YOUR CODE ENDS HERE ========== #\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "        # ========== YOUR CODE ENDS HERE ========== #\n",
    "        return mu, log_std\n",
    "\n",
    "\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, input_shape: Tuple[int, int, int], latent_size: int):\n",
    "        super().__init__()\n",
    "        assert len(input_shape) == 3\n",
    "        # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "        # ========== YOUR CODE ENDS HERE ========== #\n",
    "    \n",
    "    def reconstruction_loss(self, x: torch.Tensor, x_recon: torch.Tensor) -> torch.Tensor: \n",
    "        # compute reconstruction loss as MSE between x and x_recon\n",
    "        # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "        # ========== YOUR CODE ENDS HERE ========== #\n",
    "        return recon_loss\n",
    "    \n",
    "    def kl_loss(self, mu: torch.Tensor, log_std: torch.Tensor) -> torch.Tensor:\n",
    "        # compute kl divergence between approximate posterior N(mu, Sigma) and prior N(0, I)\n",
    "        # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "        # ========== YOUR CODE ENDS HERE ========== #\n",
    "        return kl_loss\n",
    "\n",
    "    def loss(self, x: torch.Tensor) -> OrderedDict[str, torch.Tensor]:\n",
    "        # calculate loss as sum of reconstruction and kl-divergence losses\n",
    "        # convert images to the [-1, 1] range\n",
    "        # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "        # ========== YOUR CODE ENDS HERE ========== #\n",
    "        return OrderedDict(loss=recon_loss + kl_loss, recon_loss=recon_loss,\n",
    "                           kl_loss=kl_loss)\n",
    "\n",
    "    def sample(self, n: int) -> np.ndarray:\n",
    "        # implement sampling\n",
    "        # convert images back from [-1, 1] scale to the [0, 1] scale\n",
    "        with torch.no_grad():\n",
    "            # ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "            # ========== YOUR CODE ENDS HERE ========== #\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2.2 CNN VAE training (0.25 pts)\n",
    "\n",
    "Train CNN VAE on the images data. \n",
    "\n",
    "Note: on my laptop (Macbook M1 Pro) training took ~17 mins. If you want faster training, please do this lab in colab with GPU and move your model and data to the GPU ([.cuda()](https://pytorch.org/docs/stable/generated/torch.Tensor.cuda.html#torch.Tensor.cuda) call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = (np.transpose(train_data, (0, 3, 1, 2)) / 255.).astype('float32')\n",
    "test_data = (np.transpose(test_data, (0, 3, 1, 2)) / 255.).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvVAE((3, 32, 32), 16)\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=128)\n",
    "train_losses, test_losses = train(model, train_loader, test_loader, epochs=20, lr=1e-3)\n",
    "train_losses = np.stack((train_losses['loss'], train_losses['recon_loss'], train_losses['kl_loss']), axis=1)\n",
    "test_losses = np.stack((test_losses['loss'], test_losses['recon_loss'], test_losses['kl_loss']), axis=1)\n",
    "plot_vae_training_plot(train_losses, test_losses, title=\"Train plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2.3 CNN VAE Sampling (0.25 pts)\n",
    "\n",
    "Sample 100 images using your CNN VAE model. Visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "# ========== YOUR CODE ENDS HERE ========== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2.4 CNN VAE Interpolation (0.5 pts)\n",
    "\n",
    "Take 10 image pairs ($I_1, I_2$) (20 images in total) from the test dataset. Convert them to latent codes ($z_1, z_2$) via VAE Encoder. For each pair, calculate 10 interpolations (including $z_1$ and $z_2$) of latent codes, which are distributed uniformly along the $z_1 \\rightarrow z_2$ line. Decode this interpolations and visualize them. Explain results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= YOUR CODE STARTS HERE ========= #\n",
    "\n",
    "# ========== YOUR CODE ENDS HERE ========== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Summary (0.25 pts)\n",
    "\n",
    "* Summarize in a few sentences what you have learned and achieved by completing the tasks of this assignment. Comment on how this assignment can be imporved in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Hierarchical VAE (Additional!) (up to 1.5 pts)\n",
    "\n",
    "In this part, we will explore a simplified version of the hierarchical VAE described in [NVAE](https://arxiv.org/pdf/2007.03898.pdf). We will not implement the full NVAE, but rather use some ideas from the paper to explore how to learn a prior distribution $p_Z(z)$.\n",
    "\n",
    "Implement a hierarchical VAE that follows the following structure.\n",
    "* $z1$ is a 2x2x12 latent vector where $p(z_1)$ is the unit Gaussian.\n",
    "    * Learn the approximate posterior $q_\\phi(z|x) = N(z; \\mu_\\phi(x), \\Sigma_\\phi(x))$, where $\\mu_\\phi(x)$ is the mean vector, and $\\Sigma_\\phi(x)$ is a diagonal covariance matrix. I.e., same as a normal VAE, but use a matrix latent rather than a vector. Each dimension is independent.\n",
    "* $z2$ is a 2x2x12 latent vector.\n",
    "    * $p_\\theta(z2|z1)$ is learned, and implemented as a neural network that parameterizes mean (and log std, optionally).\n",
    "    * $q_\\theta(z2|z1,x)$ is also learned. Implement this as a Residual Normal [see NVAE] over the prior $p_\\theta(z2|z1)$.\n",
    "* The decoder should be a function of $z2$ only.\n",
    "\n",
    "Some helpful hints:\n",
    "* Two KL losses should be calculated. The first should match $q_\\phi(z|x)$ to the unit Gaussian. The second should match $q_\\phi(z2|z1,x)$ and $p_\\theta(z2|z1)$, and be taken with respect to $q$.\n",
    "* When calculating the second KL term, utilize the analytic form for the residual normal. When $q_\\phi(z2|z1,x) = N(z2; \\mu_\\phi(z1) + \\Delta \\mu_\\phi(z1,x), \\Sigma_\\phi(z1)) * \\Delta \\Sigma_\\phi(z1,x))$, use the following form: `kl_z2 = -z2_residual_logstd - 0.5 + (torch.exp(2 * z2_residual_logstd) + z2_residual_mu ** 2) * 0.5`\n",
    "* When calculating KL, remember to sum over the dimensions of the latent variable before taking the mean over batch.\n",
    "* For the prior $p_\\theta(z2|z1)$, fix standard deviation to be 1. Learn only the mean. This will help with stability in training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
